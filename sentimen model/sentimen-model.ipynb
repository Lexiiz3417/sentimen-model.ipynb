{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VGpmmW6Cba3t"
      ],
      "authorship_tag": "ABX9TyMbBkNEyh4u1Br0ajsY6jTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lexiiz3417/sentimen-model.ipynb/blob/main/sentimen_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Persiapan (Install Library & Download Data NLTK)\n",
        "Ini langkah awal buat install dan unduh semua yang dibutuhkan."
      ],
      "metadata": {
        "id": "cn1HCdb9adxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk scikit-learn tensorflow joblib\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import joblib\n",
        "import nltk\n",
        "\n",
        "# Unduh data stopwords Indonesia dari NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "zbj-YPTda0oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Muat & Bersihkan Data\n",
        "Blok ini berfungsi buat ngambil data dari CSV dan ngebersihin teksnya."
      ],
      "metadata": {
        "id": "VnJiXjt3ahV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Corpus Bahasa Indonesia (Label).csv')\n",
        "\n",
        "# Perbaikan untuk mengatasi KeyError: 'teks'\n",
        "data.rename(columns={'kalimat': 'teks', 'sentiment': 'sentiment'}, inplace=True)\n",
        "\n",
        "# Ganti nilai sentiment 2 menjadi 0 (Negatif/Netral)\n",
        "# Jika ingin 3 kelas, jangan jalankan baris ini dan pastikan label y-nya 0, 1, 2\n",
        "# data['sentiment'] = data['sentiment'].replace(2, 0)\n",
        "\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in list_stopwords]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "data['teks_bersih'] = data['teks'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "hB4BsV1HbJVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek nilai unik di kolom sentiment\n",
        "print(\"\\nNilai unik di kolom 'sentiment':\")\n",
        "print(data['sentiment'].unique())\n",
        "\n",
        "# Cek tipe data dari kolom sentiment\n",
        "print(\"\\nTipe data kolom 'sentiment':\")\n",
        "print(data['sentiment'].dtype)"
      ],
      "metadata": {
        "id": "0gQ5_CWUdr9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Ubah Teks ke Angka (TF-IDF)\n",
        "Di sini kita ubah teks yang udah bersih jadi matriks angka."
      ],
      "metadata": {
        "id": "6bhNV5ilbOyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(data['teks_bersih'])\n",
        "\n",
        "print(\"\\nShape dari matriks TF-IDF:\")\n",
        "print(vectors.shape)"
      ],
      "metadata": {
        "id": "_8IKZbUYbXNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kode untuk menyimpan vectorizer\n",
        "!pip install joblib\n",
        "import joblib\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')"
      ],
      "metadata": {
        "id": "QI-ca7DS0Rp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Bagi Data & One-Hot Encoding\n",
        "Blok ini memisahkan data menjadi dua bagian, untuk diajarin dan dites."
      ],
      "metadata": {
        "id": "VGpmmW6Cba3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Ambil kolom sentiment sebagai label (y)\n",
        "X = vectors\n",
        "y = data['sentiment']\n",
        "\n",
        "# Ubah label (0, 1, 2) menjadi one-hot encoding\n",
        "y_one_hot = to_categorical(y)\n",
        "\n",
        "# Bagi data menjadi 80% untuk latih dan 20% untuk uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nData berhasil dibagi dan diubah menjadi One-Hot Encoding!\")\n",
        "print(f\"Jumlah data latih: {X_train.shape[0]}\")\n",
        "print(f\"Jumlah data uji: {X_test.shape[0]}\")\n",
        "print(f\"Bentuk label y_train: {y_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC4FfahgbgVc",
        "outputId": "afd74661-7dae-4aab-d02a-c51b98eb439f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data berhasil dibagi dan diubah menjadi One-Hot Encoding!\n",
            "Jumlah data latih: 10087\n",
            "Jumlah data uji: 2522\n",
            "Bentuk label y_train: (10087, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Bangun Model JST Multi-Kelas\n",
        "Ini bagian inti di mana kita buat arsitektur JST-nya."
      ],
      "metadata": {
        "id": "WQw-KBfYblV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bangun Model JST Sederhana\n",
        "model = Sequential()\n",
        "\n",
        "# Lapisan pertama\n",
        "model.add(Dense(16, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "\n",
        "# Lapisan output (diganti jadi 3 neuron dan softmax)\n",
        "# 3 = jumlah kelas\n",
        "# 'softmax' = fungsi aktivasi untuk multi-kelas\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Tampilkan ringkasan model\n",
        "model.summary()\n",
        "\n",
        "# Kompilasi Model\n",
        "# 'categorical_crossentropy' = fungsi loss untuk multi-kelas\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Tr0asow1brNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Latih Model\n",
        "Blok terakhir ini buat 'ngajarin' model JST-mu pakai data latih."
      ],
      "metadata": {
        "id": "g4hL88dYbt8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latih model\n",
        "# 'epochs' adalah berapa kali model belajar dari seluruh data\n",
        "# 'batch_size' adalah jumlah sampel per langkah pelatihan\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "1W5E9AkXbzBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.  Simpan Model AI"
      ],
      "metadata": {
        "id": "GdKgaQffb10R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model ke dalam file\n",
        "model.save('sentimen_model.h5')\n",
        "\n",
        "# Modelmu sekarang tersimpan dengan nama 'sentimen_model.h5' di Colab!\n",
        "# Kamu bisa download file ini dari menu di samping kiri Colab"
      ],
      "metadata": {
        "id": "Qu5SNdgdgFZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
