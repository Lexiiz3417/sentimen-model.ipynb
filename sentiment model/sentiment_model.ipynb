{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VGpmmW6Cba3t"
      ],
      "mount_file_id": "1MFIXB36EnDMOAzW1upT4C_PNbxCGf8dK",
      "authorship_tag": "ABX9TyPV7sEbMfmS0+hG5p5KpRNK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lexiiz3417/sentiment-analysis/blob/main/sentimen%20model\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Persiapan (Install Library & Download Data NLTK)\n",
        "Ini langkah awal buat install dan unduh semua yang dibutuhkan."
      ],
      "metadata": {
        "id": "cn1HCdb9adxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk scikit-learn tensorflow joblib\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import joblib\n",
        "import nltk\n",
        "\n",
        "# Unduh data stopwords Indonesia dari NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "zbj-YPTda0oj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec14287f-6b3a-4ab0-fdb1-55255c310ca4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Muat & Bersihkan Data\n",
        "Blok ini berfungsi buat ngambil data dari CSV dan ngebersihin teksnya.\n",
        "[Download datasetnya disini](https://www.kaggle.com/datasets/achmadshidiq/corpus-bahasa-indonesia-label/data)"
      ],
      "metadata": {
        "id": "VnJiXjt3ahV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Corpus Bahasa Indonesia (Label).csv')\n",
        "\n",
        "# Perbaikan untuk mengatasi KeyError: 'teks'\n",
        "data.rename(columns={'kalimat': 'teks', 'sentiment': 'sentiment'}, inplace=True)\n",
        "\n",
        "# Ganti nilai sentiment 2 menjadi 0 (Negatif/Netral)\n",
        "# Jika ingin 3 kelas, jangan jalankan baris ini dan pastikan label y-nya 0, 1, 2\n",
        "# data['sentiment'] = data['sentiment'].replace(2, 0)\n",
        "\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in list_stopwords]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "data['teks_bersih'] = data['teks'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "hB4BsV1HbJVX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek nilai unik di kolom sentiment\n",
        "print(\"\\nNilai unik di kolom 'sentiment':\")\n",
        "print(data['sentiment'].unique())\n",
        "\n",
        "# Cek tipe data dari kolom sentiment\n",
        "print(\"\\nTipe data kolom 'sentiment':\")\n",
        "print(data['sentiment'].dtype)"
      ],
      "metadata": {
        "id": "0gQ5_CWUdr9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ff0b51-e908-49c2-8664-5f89a1b47d3d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Nilai unik di kolom 'sentiment':\n",
            "[0 1 2]\n",
            "\n",
            "Tipe data kolom 'sentiment':\n",
            "int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Ubah Teks ke Angka (TF-IDF)\n",
        "Di sini kita ubah teks yang udah bersih jadi matriks angka."
      ],
      "metadata": {
        "id": "6bhNV5ilbOyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(data['teks_bersih'])\n",
        "\n",
        "print(\"\\nShape dari matriks TF-IDF:\")\n",
        "print(vectors.shape)"
      ],
      "metadata": {
        "id": "_8IKZbUYbXNL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00da3e0-5d4b-4d7e-a818-055f9808b018"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape dari matriks TF-IDF:\n",
            "(12609, 13304)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kode untuk menyimpan vectorizer\n",
        "!pip install joblib\n",
        "import joblib\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')"
      ],
      "metadata": {
        "id": "QI-ca7DS0Rp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b36588e-e13d-4e1e-c120-755ecc80ed02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Bagi Data & One-Hot Encoding\n",
        "Blok ini memisahkan data menjadi dua bagian, untuk diajarin dan dites."
      ],
      "metadata": {
        "id": "VGpmmW6Cba3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Ambil kolom sentiment sebagai label (y)\n",
        "X = vectors\n",
        "y = data['sentiment']\n",
        "\n",
        "# Ubah label (0, 1, 2) menjadi one-hot encoding\n",
        "y_one_hot = to_categorical(y)\n",
        "\n",
        "# Bagi data menjadi 80% untuk latih dan 20% untuk uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nData berhasil dibagi dan diubah menjadi One-Hot Encoding!\")\n",
        "print(f\"Jumlah data latih: {X_train.shape[0]}\")\n",
        "print(f\"Jumlah data uji: {X_test.shape[0]}\")\n",
        "print(f\"Bentuk label y_train: {y_train.shape}\")"
      ],
      "metadata": {
        "id": "ZC4FfahgbgVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6bf6b95-6304-462b-ecbd-3246acb67f5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data berhasil dibagi dan diubah menjadi One-Hot Encoding!\n",
            "Jumlah data latih: 10087\n",
            "Jumlah data uji: 2522\n",
            "Bentuk label y_train: (10087, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Bangun Model JST Multi-Kelas\n",
        "Ini bagian inti di mana kita buat arsitektur JST-nya."
      ],
      "metadata": {
        "id": "WQw-KBfYblV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bangun Model JST Sederhana\n",
        "model = Sequential()\n",
        "\n",
        "# Lapisan pertama\n",
        "model.add(Dense(16, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "\n",
        "# Lapisan output (diganti jadi 3 neuron dan softmax)\n",
        "# 3 = jumlah kelas\n",
        "# 'softmax' = fungsi aktivasi untuk multi-kelas\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Tampilkan ringkasan model\n",
        "model.summary()\n",
        "\n",
        "# Kompilasi Model\n",
        "# 'categorical_crossentropy' = fungsi loss untuk multi-kelas\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Tr0asow1brNo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "cd4d1323-3288-4de9-b7cc-c78eaf909cb3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │       \u001b[38;5;34m212,880\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">212,880</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m212,931\u001b[0m (831.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,931</span> (831.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m212,931\u001b[0m (831.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,931</span> (831.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Latih Model\n",
        "Blok terakhir ini buat 'ngajarin' model JST-mu pakai data latih."
      ],
      "metadata": {
        "id": "g4hL88dYbt8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latih model\n",
        "# 'epochs' adalah berapa kali model belajar dari seluruh data\n",
        "# 'batch_size' adalah jumlah sampel per langkah pelatihan\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "1W5E9AkXbzBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455e36ce-9eff-457e-f72e-95c9dd3ca197"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.4997 - loss: 1.0492 - val_accuracy: 0.5036 - val_loss: 0.9643\n",
            "Epoch 2/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5835 - loss: 0.8553 - val_accuracy: 0.6360 - val_loss: 0.8315\n",
            "Epoch 3/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.8135 - loss: 0.6011 - val_accuracy: 0.6614 - val_loss: 0.7633\n",
            "Epoch 4/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - accuracy: 0.8885 - loss: 0.4320 - val_accuracy: 0.6681 - val_loss: 0.7507\n",
            "Epoch 5/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9192 - loss: 0.3246 - val_accuracy: 0.6733 - val_loss: 0.7649\n",
            "Epoch 6/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9324 - loss: 0.2602 - val_accuracy: 0.6745 - val_loss: 0.7914\n",
            "Epoch 7/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9381 - loss: 0.2151 - val_accuracy: 0.6733 - val_loss: 0.8230\n",
            "Epoch 8/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9540 - loss: 0.1752 - val_accuracy: 0.6729 - val_loss: 0.8583\n",
            "Epoch 9/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9524 - loss: 0.1592 - val_accuracy: 0.6768 - val_loss: 0.9017\n",
            "Epoch 10/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9565 - loss: 0.1436 - val_accuracy: 0.6745 - val_loss: 0.9431\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7db3a16d7f90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.  Simpan Model AI"
      ],
      "metadata": {
        "id": "GdKgaQffb10R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model ke dalam file\n",
        "model.save('sentimen_model.h5')\n",
        "\n",
        "# Modelmu sekarang tersimpan dengan nama 'sentimen_model.h5' di Colab!\n",
        "# Kamu bisa download file ini dari menu di samping kiri Colab"
      ],
      "metadata": {
        "id": "Qu5SNdgdgFZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1998da34-b6b6-405f-e2f9-f76ab8a9d052"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}